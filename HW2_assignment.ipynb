{"cells":[{"cell_type":"markdown","metadata":{"id":"zeiS-3kVYS0r"},"source":["# ORIE 5355/INFO 5370 HW 2: Recommendation systems"]},{"cell_type":"markdown","metadata":{"id":"0FTlLbUhYS0t"},"source":[" - Name:\n"," - Net-id:\n"," - Date: \n"," - Late days used for this assignment:\n"," - Total late days used (counting this assignment):\n"," - People with whom you discussed this assignment: "]},{"cell_type":"markdown","metadata":{"id":"_D7xZ4C5YS0u"},"source":["After you finish the homework, please complete the following (short, anonymous) post-homework survey: https://forms.gle/7tFZUoDszbDeDKmV6 and include the survey completion code below."]},{"cell_type":"markdown","metadata":{"id":"WVYikyPsYS0u"},"source":["### Question 0 [2 points]\n","<font color='blue'> Survey completion code: "]},{"cell_type":"markdown","metadata":{"id":"MBjHArXvYS0u"},"source":["We have marked questions in <font color='blue'> blue </font>. Please put answers in black (do not change colors). You'll want to write text answers in \"markdown\" mode instead of code. In Jupyter notebook, you can go to Cell > Cell Type > Markdown, from the menu. Please carefully read the late days policy and grading procedure [here](https://orie5355.github.io/Fall_2021/assignments/). "]},{"cell_type":"markdown","metadata":{"id":"vlW-Uf5zYS0v"},"source":["# Conceptual component"]},{"cell_type":"markdown","metadata":{"id":"xKHHlNTxYS0v"},"source":["Go through the \"Algorithms tour\" [here](https://algorithms-tour.stitchfix.com/). It's a great view of the combination of algorithms used by a modern e-commerce company. "]},{"cell_type":"markdown","metadata":{"id":"ky46xJSAYS0v"},"source":["<font color='blue'> 1) How do they use a combination of \"latent\" factors and explicit features to gain the benefits of collaborative filtering (matrix factorization) while not being susceptible to cold start issues?"]},{"cell_type":"markdown","source":["They have a lot of explicit data, both from clients' self-descriptions and from clothing attributes. "],"metadata":{"id":"KkJbbZPOeRAc"}},{"cell_type":"markdown","metadata":{"id":"yeYuSsWbYS0w"},"source":["<font color='blue'> 2) How do they match clients with human stylists who make the final decision? Does it remind you of anything we learned in class?"]},{"cell_type":"markdown","source":["This match score is a complex function of the history between that client and stylist (if any), and the affinities between the client's stated and latent style preferences and those of the stylist.\n","\n","This relats to the “Cold start” with matrix factorization we learnt in class. It combines matrix factorization with user-similarity based approaches.\n","\n"],"metadata":{"id":"zS9n84oTgjCc"}},{"cell_type":"markdown","metadata":{"id":"gZHRlryYYS0x"},"source":["<font color='blue'> 3) How do they manage their inventory to ensure that they have enough items that future customers will want?"]},{"cell_type":"markdown","source":["They answer these questions by using a model of the system dynamics, fitting it to historical data and using it for robust optimization given quantified uncertainties in our forecasts."],"metadata":{"id":"bUgJ16k8qDmY"}},{"cell_type":"markdown","metadata":{"id":"P5bMoo6LYS0x"},"source":["# Programming component"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SUc0KagYY_8","executionInfo":{"status":"ok","timestamp":1664238336825,"user_tz":240,"elapsed":909,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}},"outputId":"c46ff5d9-83a6-42f0-f43d-946cdf2c48d4"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","path = \"/content/drive/My Drive/Colab Notebooks/\"\n","\n","os.chdir(path)\n","\n","os.listdir(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGvwOw21TFHz","executionInfo":{"status":"ok","timestamp":1664238924297,"user_tz":240,"elapsed":158,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}},"outputId":"f79249fe-a00c-4c42-d40c-c1affe2d2f69"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['6.1 CS 5785 Applied ML_A0.ipynb',\n"," '“cornell-cs5785-python-tutorial.ipynb”的副本',\n"," 'Lecture 2: Supervised Machine Learning.ipynb',\n"," 'Intro to Data Analysis.ipynb',\n"," 'data']"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"-99RkHFZYS0y"},"source":["## Helper code"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"aYvkPSZ9YS0y","executionInfo":{"status":"ok","timestamp":1664239123575,"user_tz":240,"elapsed":189,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os, sys, math\n","import matplotlib.pyplot as plt\n","import pickle\n","def load_pickle(filename):\n","    with open(filename, \"rb\") as f:\n","        data = pickle.load(f)\n","    return data\n","\n","def load_ratings_and_factors(type_name = 'interaction'):\n","    ratings = load_pickle('data/{}_ratings'.format(type_name))\n","    book_vectors = load_pickle('data/{}_dict_book_factor'.format(type_name))\n","    user_vectors = load_pickle('data/{}_dict_reader_factor'.format(type_name))\n","    return ratings, book_vectors, user_vectors"]},{"cell_type":"markdown","metadata":{"id":"q0YZNqarYS0y"},"source":["**In** this homework, we are giving you trained user and book item vectors using a GoodReads dataset. Goodreads is a social cataloging website that allows individuals to search its database of books, annotations, quotes, and reviews. There are multiple types of interactions that a user can have with a book: add books to a list of books they intend to read (\"short-list\" the book), indicate they have read books before, and review books they have read. \n","\n","Here, we work with multiple types of interactions as training data for a recommendation system. For each \"type\" of rating data, **we give you the raw ratings data, as well as user and item vectors trained using a Python package** (https://berkeley-reclab.github.io/) that implements matrix factorization in cases where there are missing entries in a matrix. The \"ratings\" data is in a \"sparse matrix\"/dictionary format, meaning that the dictionary keys are of the kind (user, item), and the dictionary value is the corresponding value. **Not all pairs are in the matrix, indicating that that value is missing or at its default value**. \n","\n","There are 4 types of rating/interaction data:\n","\n"," - `Interaction`: a \"1\" indicates the user has interacted with the book at some point in the past, either by saying that they intend to read it, have read it, or have given it a rating. **If it is missing**, that means the user has not interacted with the book.\n"," \n"," - `Explicit Rating`: explicit ratings. Numeric values indicate the ratings given. If it is missing, that means the user has not rated the book.\n","  \n"," - `Rating_all_zero`: explicit ratings. Numeric values more than 0 indicate the ratings given. **Now, we replace missing values from above with \"zeros,\" so that there are no missing ratings.**\n"," \n"," - `Rating_interaction_zero`: explicit ratings. Numeric values more than 0 indicate the ratings given. Now, we replace missing values from above with \"zeros,\" **only if the user interacted with that book in the past**."]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"881sKaIIYS0z","executionInfo":{"status":"error","timestamp":1664239127094,"user_tz":240,"elapsed":133,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}},"outputId":"3b6f1a74-2e60-40f4-f722-c69728509645"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-1a0132a707df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_interactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ratings_and_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'interaction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mratings_explicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_explicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_explicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ratings_and_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mratings_allmissing0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_allmissing0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_allmissing0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ratings_and_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rating_all_zero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mratings_interact0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_interact0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_interact0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ratings_and_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rating_interaction_zero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-71-5322ac19a253>\u001b[0m in \u001b[0;36mload_ratings_and_factors\u001b[0;34m(type_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/{}_ratings'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbook_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/{}_dict_book_factor'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0muser_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/{}_dict_reader_factor'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-71-5322ac19a253>\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/interaction_dict_reader_factor'"]}],"source":["ratings_interactions, book_vectors_interactions, user_vectors_interactions = load_ratings_and_factors(type_name = 'interaction')\n","ratings_explicit, book_vectors_explicit, user_vectors_explicit = load_ratings_and_factors(type_name = 'rating')\n","ratings_allmissing0, book_vectors_allmissing0, user_vectors_allmissing0 = load_ratings_and_factors(type_name = 'rating_all_zero')\n","ratings_interact0, book_vectors_interact0, user_vectors_interact0 = load_ratings_and_factors(type_name = 'rating_interaction_zero')\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"sEmlO6Z4YS0z","executionInfo":{"status":"ok","timestamp":1664238441500,"user_tz":240,"elapsed":162,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}}},"outputs":[],"source":["def get_shapes_and_ranges(ratings, book_vectors, item_vectors):\n","    print(len(ratings), np.shape(book_vectors), np.shape(item_vectors), min(ratings.values()), max(ratings.values()))"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"m2lNTUOkYS0z","outputId":"55e1010c-e9c2-49bf-d368-fd0f4e78fe6b","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"error","timestamp":1664238443698,"user_tz":240,"elapsed":132,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-6e1e539153f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_shapes_and_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_interactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_shapes_and_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_explicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_explicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_explicit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_shapes_and_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_allmissing0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_allmissing0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_allmissing0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_shapes_and_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_interact0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_vectors_interact0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vectors_interact0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ratings_interactions' is not defined"]}],"source":["get_shapes_and_ranges(ratings_interactions, book_vectors_interactions, user_vectors_interactions)\n","get_shapes_and_ranges(ratings_explicit, book_vectors_explicit, user_vectors_explicit)\n","get_shapes_and_ranges(ratings_allmissing0, book_vectors_allmissing0, user_vectors_allmissing0)\n","get_shapes_and_ranges(ratings_interact0, book_vectors_interact0, user_vectors_interact0)"]},{"cell_type":"markdown","metadata":{"id":"hR_pq7fHYS00"},"source":["## Problem 1: Predictions and recommendations with different data types"]},{"cell_type":"markdown","metadata":{"id":"Ng0d4g8vYS00"},"source":["### 1a) What do different data types mean?"]},{"cell_type":"markdown","metadata":{"id":"Wc_q8npDYS00"},"source":["<font color='blue'> What is `Rating_interaction_zero` trying to capture -- why would we fill in books that someone interacted with but did not rate as a 0? (Hint: connect to conceptual reading from HW1). Answer in no more than 3 sentences. "]},{"cell_type":"markdown","metadata":{"id":"ca1ciuGwYS00"},"source":["Those who interact with the book but didn't gave a rating.\n","\n","This group could tend to not like this book so didn't rate it. Could cause selection bias."]},{"cell_type":"markdown","metadata":{"id":"m1asfXfWYS00"},"source":["<font color='blue'> What are some potential problems you see with using `rating_all_zero` for recommendations? Answer in no more than 3 sentences.\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"vhdsPgG7YS00"},"source":["It neglect those who interact with the book but didn't gave a rating. \n","\n","It also missed out on the latent data that could tell us something about the users or books. (Some users like to interact but not rate. Some books was so bad or users have mixed feeling about it, so they tend not to rate.) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"B6nDyikFYS01"},"source":["### 1b) Generating predictions"]},{"cell_type":"markdown","metadata":{"id":"lR1NXHRAYS01"},"source":["<font color='blue'> Fill in the following function that takes in a user matrix (where each row is 1 user vector) and an item matrix (where each row is 1 item vector), and returns a matrix of predicted ratings for each user and item, where each entry is associated with the corresponding user (row number) and item (column number)"]},{"cell_type":"code","source":["ratings_interactions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"bLblv_P5S5N2","executionInfo":{"status":"error","timestamp":1664237676317,"user_tz":240,"elapsed":173,"user":{"displayName":"Yubang Wu","userId":"18068770298306590806"}},"outputId":"fc15b5bc-596a-4f18-a15f-37f36d503f68"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-50013946f9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings_interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ratings_interactions' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APWA365rYS01"},"outputs":[],"source":["def get_predictions(user_vectors, book_vectors):\n","    pass # your code here\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_pq-dTAYS01"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nxyqpUSDYS01"},"source":["<font color='blue'> Output the predictions for first 10 items for the first user, using each of the 4 data types.\n","\n","For example, the predictions for one of the data types are:\n","\n","Ratings for first 10 items,  interactions:\n","[-0.003  0.01   0.002 -0.001  0.003  0.007 -0.01   0.007  0.001  0.003]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COLzZRpPYS01"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Qes2rh1EYS01"},"source":["<font color='blue'> Do a scatterplot of the predicted rating for the \"interaction\" and \"explicit ratings\" types. (Each dot represents one user and one book, with X axis being predicted ratings using interaction data and Y axis being predicted rating using explicit ratings). Describe what you see in no more than 2 sentences. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOeZpvoHYS01"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"eKszsldIYS01"},"source":["### 1c) From predictions to recommendations (without capacity constraints)"]},{"cell_type":"markdown","metadata":{"id":"m5Py5We5YS02"},"source":["<font color='blue'> Fill in the following function that takes in the matrix of predicted ratings for each user and item, and returns a dictioanry where the keys are the user indices and the values are a list of length \"number_top_items\" indicating the recommendations given to that user"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3SZlhdOYS02"},"outputs":[],"source":["def get_recommendations_for_each_user(predictions, number_top_items = 10):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"P4Pk-uHPYS02"},"source":["<font color='blue'> Output the recommendations for the first user, using each of the 4 data types.\n","\n","For example, from the \"Interaction\" dataset, you should get: [182, 198, 19, 100, 104, 73, 30, 199, 164, 74]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ama9Bl7MYS02"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"p2vO3jnzYS02"},"source":["<font color='blue'> Fill in the following function that takes in the (top 10) recommendations for each user, and outputs a histogram for how often each item is to be recommended. For example, if there are 18 items, and 10 of them were never recommended, 5 of them were recommended once each, and 3 of them were recommended five times each, then you would have bars at 0, 1, and 5, of height 10, 5, and 3, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDouudaTYS02"},"outputs":[],"source":["def show_frequency_histograms(recommendations):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"4V-IYv1gYS02"},"source":["<font color='blue'> Show the histograms for the \"interact\" and \"explicit\" data types. Describe what you observe in no more than 3 sentences. For example, discuss how often is the most recommended item recommended, how that compares to the least recommended items, and what that could mean for recommendations in various contexts. "]},{"cell_type":"markdown","metadata":{"id":"4jOMWbLfYS02"},"source":["# Problem 2: Cold start -- recommendations for new users"]},{"cell_type":"markdown","metadata":{"id":"utnlMzssYS02"},"source":["In this part of the assignment, we are going to ask you to tackle the \"cold-start\" problem with matrix-factorization based recommendation systems. The above recommendation techniques worked when you had access to past data for reach user, such as interactions or explicit ratings. However, it doesn't work as well when a new user has just joined the platform and so the platform doesn't have any data.  \n","\n","You should also see a comma-separated values file (user_demographics.csv) that contains basic demographic information on each user. Each row describes one user, and have four attributes: 'User ID', 'Wealth', 'Age group' and 'Location'.\n","\n","User ID is the unique identifier associated with each user, and it is in the same order as the user_vectors, and in the same indexing as the ratings (be careful about 0 and 1 indexing in Python).\n","\n","Wealth is a non-negative, normalized value indicating the average wealth of the neighborhood in which the user is, where we normalized it such that each Location has similar wealth distributions. Age group describes the age of the user. Location describes the region that the user is from."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7U-fVJ16YS02","outputId":"3a09af57-89f9-4091-9f37-aed03fc8787e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Wealth</th>\n","      <th>Age group</th>\n","      <th>Location</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1.833101</td>\n","      <td>50 to 64</td>\n","      <td>America</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2.194996</td>\n","      <td>18 to 34</td>\n","      <td>America</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>2.216195</td>\n","      <td>18 to 34</td>\n","      <td>Europe</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.838690</td>\n","      <td>50 to 64</td>\n","      <td>Asia Pacific</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>2.109313</td>\n","      <td>18 to 34</td>\n","      <td>America</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   User ID    Wealth Age group      Location\n","0        1  1.833101  50 to 64       America\n","1        2  2.194996  18 to 34       America\n","2        3  2.216195  18 to 34        Europe\n","3        4  0.838690  50 to 64  Asia Pacific\n","4        5  2.109313  18 to 34       America"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["demographics = pd.read_csv(\"data/user_demographics.csv\")\n","demographics.head()"]},{"cell_type":"markdown","metadata":{"id":"Wegx2G60YS03"},"source":["We are now going to pretend that we don't have the personalized ratings/interactions history for the last 100 users, and thus don't have their user vectors. Rather, let's pretend that these are new users to the platform, and you are able to get the above demographics from their browswer cookies/IP address. Now, we're going to try to recommend items for them anyway. For this part, we'll exclusively use the \"ratings with interaction0\" data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4LdypmyYS03"},"outputs":[],"source":["existing_user_vectors = user_vectors_interact0[0:900,:]\n","existing_user_demographics = demographics.iloc[0:900,:]\n","new_user_demographics = demographics.iloc[900:,:]"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"yokv__0rYS03"},"source":["### 2a) Predictions for new users [Simple]"]},{"cell_type":"markdown","metadata":{"id":"PFes2AW8YS03"},"source":["<font color='blue'> Fill in the following function that takes in: the demographics of a single new user, the demographics of all the existing users in your platform, and the user vectors of all the existing users, and outputs a 'predicted' user vector for the new user to use until we get enough data for that user. \n","    \n","<font color='blue'> For this question, we ask you to use the following simple method to construct the vector for the new user. Each user is classified as \"Low\" or \"High\" wealth based on whether their Wealth score is below or above the median of 1.70. Then, we simply construct a mean user vector for \"Low\" and \"High\" wealth, based on the 900 users (take the average vector among users with \"Low\" and \"High\" Wealth, respectively.). The correpsonding mean vector is then used for each new user. \n","\n","For example, using this method, you should find that the vector for the second user (index \"1\") is:\n","\n","array([-0.183, -0.149, -0.141, -0.199, -0.166, -0.272, -0.02 ,  0.137,\n","       -0.12 ,  0.022])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFsyvGC7YS03","outputId":"1d29a6e9-1eab-4713-908b-5b5b5b3b253f"},"outputs":[{"data":{"text/plain":["1.7026180771992308"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["existing_user_demographics.Wealth.median()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQgNuMq-YS03"},"outputs":[],"source":["def get_user_vector_for_new_user(new_user, existing_user_demographics, existing_user_vectors):\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-nQzrdQYS03"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"M13Z9rDKYS03"},"source":["<font color='blue'> Output the mean vector predicted for the first user (index 0) in `new_user_demographics`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozOb-sTFYS04"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"YE8ITFsRYS04"},"source":["### 2b) [Bonus, 3 points] Predictions for new users [Using KNN or another model]"]},{"cell_type":"markdown","metadata":{"id":"mSGiqqsUYS04"},"source":["<font color='blue'> Fill in the following function that takes in: the demographics of a single new user, the demographics of all the existing users in your platform, and the user vectors of all the existing users, and outputs a 'predicted' user vector for the new user to use until we get enough data for that user. \n","    \n","<font color='blue'> Now, use K nearest neighbors or some other machine learning method. \n","    \n","<font color='blue'> Feel free to prepare data/train a model outside this function, and then use your trained model within the function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XhnMmSiYS04"},"outputs":[],"source":["def get_user_vector_for_new_user_knn(new_user, existing_user_demographics, existing_user_vectors):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"az1KcszbYS04"},"source":["<font color='blue'> Output the mean vector predicted for the first user in `new_user_demographics`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGujm9EsYS04"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lGYhEoR7YS04"},"source":["<font color='blue'> Justify your choice of model. If you used K nearest neighbors, then how did you decide upon your distance function? If you used another model, how does that model weight the different demographics in importance (either implicitly or explicitly)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnj6FN-hYS04"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Ih7oEGg3YS04"},"source":["### 2c) Comparing predictions from \"true\" user vector and from above"]},{"cell_type":"markdown","metadata":{"id":"rivnGJEPYS04"},"source":["<font color='blue'> For each of the 100 \"new\" users, use either your model from 2a or 2b (\"demographic model\") to retrieve a user vector for that user, and then your functions from Problem 1 to get predicted ratings and top-10 recommendations. First, plot a scatterplot between the ratings predicted by the demographic model and the ratings predicted by the full model from Problem 1. Each point in the scatter plot should correspond to one user and one item, and so your scatterplot should have 100*200 points.  \n","\n","For example, for the first user-item pair (index 0 user, index 0 user), your prediction using the basic demographic should be -0.0011902780252621872, and using the full model should be 0.31447640890118356. So one point in the scatter plot would be (-0.0011902780252621872, 0.31447640890118356)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhpExWR-YS04"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1lkdz5Z5YS04"},"source":["<font color='blue'> Now for each new user, calculate the mean rating (according to the \"full\" model in Problem 1) for the 10 items recommended to that user, by each of the demopgraphic and \"full\" models. Output a scatterplot for the two mean ratings, where each point correpsonds to 1 user (and so you will have 100 points in your scatter plot). For example, for the first new user, the associated point is (2.4880867541832146, 0.5305243424156764). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bt1IFMqaYS05"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SMLBWQtZYS05"},"source":["<font color='blue'> Comment on the above. What is the \"loss\" from using demogprahics since we do not have access to the full data?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGPmG6TLYS05"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"13RV4k-DYS05"},"source":["# Problem 3: Predictions under capacity constraints"]},{"cell_type":"markdown","metadata":{"id":"rxmFACdkYS05"},"source":["Above, you should have observed that if we just recommend the top items for each user, some items get recommended quite a bit, and many items do not get recommended at all. Here, we are going to ask you to implement recommendations under capacity constraints.\n","\n","Throughout this part, assume that you only have 5 copies of each item that you recommend, and that you will only recommend 1 item to each user. In other words, you cannot recommend the same item more than 5 times, and so there are exactly 1000 items in stock (representing 200 unique books) for your 1000 users. \n","\n","We'll continue exclusively using the \"ratings with interaction0\" data."]},{"cell_type":"markdown","metadata":{"id":"9Hox_qkjYS05"},"source":["Now, let's assume that users are entering the platform sequentially in order of index. So the index 0 user comes first, index 1 user comes second, etc. "]},{"cell_type":"markdown","metadata":{"id":"X55PEaQHYS05"},"source":["### 3a) Naive recommendations under capacity constraints"]},{"cell_type":"markdown","metadata":{"id":"8EVgBv3qYS05"},"source":["<font color='blue'> First, let's pretend that we were naively recommending the predicted favorite item to each user. Of course, with unlimited capacity, each user would be recommended their predicted favorite. With capacity constraints, the favorite items of the users who come in later might already have reached their capacity, and so they have to be recommended an item further down their list. \n","    \n","<font color='blue'> Do the following: simulate users coming in sequentially, in order of index. For each user, recommend to them their predicted favorite item that is still available. So the first user will get their favorite item, but the last few users will almost certainly not receive any of their top few predicted items. For each user, keep track of what the rank of the item that they were ultimately recommended was, according to the predicting ranking over items for that user.\n","    \n","For example, you'll see that the first user was recommended their favorite item, but the last user was recommended their 129th favorite item. \n","\n","\n","<font color='blue'> Plot the resulting rankings in 2 ways: 1) A line plot, where the X axis is the user index and the Y axis is the rank of the item that they were recommended. and 2) A histogram of how often each rank shows up. (the X axis is the (binned) rank, and the Y axis is the count of that bin). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4h7kKyiYS05"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nvCKnFZ3YS05"},"source":["### 3b) [Bonus -- 4 pts] Optimal recommendations under capacity constraints -- maximum weight matching"]},{"cell_type":"markdown","metadata":{"id":"crz7--vvYS05"},"source":["[2 points] <font color='blue'> Now let's do \"optimal\" recommendations with capacity, using maximum weight matching. Create the same two plots as above. <font color='blue'> Describe what you observe compared to the naive recommendations above. \n","    \n","<font color='blue'>We suggest you use the `scipy.optimize.linear_sum_assignment` function. In that case, `np.tile` might also come in handy to create 5 copies of each items.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3JajbvqYS05"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ucEGn3jhYS06"},"source":["[2 points] <font color='blue'> Of course, in reality you don't observe all the users at the same time -- they come in one by one, and you need to create a recommendation for the first user before the 50th user shows up. Here's let's pretend that users show up in batches of 100. So the first 100 users at the same time, next 100, etc. In this case, you can do \"batched maximum weight matching,\" where you run maximum weight matching for the first 100 together to determine recommendations. Then, you do the same thing for the next 100 users with the items that are remaining, etc. \n","    \n","<font color='blue'> Implement the above, show the same two plots as above, and describe what you observe. Note that this part requires careful attention for how many of each item remain after each round. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4kVxfPSYS06"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4zCs4vQzYS06"},"source":["### 3c) Score functions for recommendations under capacity constraints"]},{"cell_type":"markdown","metadata":{"id":"U-wIqrJVYS06"},"source":["<font color='blue'>Here, we are working with just 200 items and 1000 users, and so batched maximum weight matching is feasible to run. In practice, with millions of items, that  might not be an effective strategy. Now, we ask you to implement the score function approach from class.\n","\n","<font color='blue'> You should normalize the predicted ratings between 0 and 1 so that you are not dividing by a negative or close to 0 average rating before proceeding.\n","    \n","<font color='blue'> Implement the above and run the same simulation as part 3c, show the same two plots, and describe what you observe."]},{"cell_type":"markdown","metadata":{"id":"ko1_-MqVYS06"},"source":["<font color='blue'> For this part, use the following score function:\n","    \n","$$\\frac{r_{ij}}{\\bar{r_{j}}} \\sqrt{C_j} $$\n","\n","HINT: In your code, for each user $i$ you will:\n","1. Retrieve the ratings ${r_{ij}}$ for each item $j$. \n","2. Normalize each  ${r_{ij}}$ for by mean item rating $r_j$ and multiply by the sqrt of the current capacity for that item. \n","3. Sort the items by the above modified score, and recommend the best item according to the modified score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4jWZLpFYS06"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0ftl7O5YS06"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6S1748xYS06"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9xGtKOOOYS06"},"source":["Comment for entire homework: In this homework, we haven't been careful with what is \"training\" data and what is \"test\" data. For example, in 3c, you're using average ratings from customers who haven't shown up yet in your simulation. In Problem 2, when training the user/book vectors we used data from customers that we are then pretending we haven't seen data from. In practice, and for the class project, you should be more careful. Such train/test/validation pipelines should be a core part of what you learn in machine learning classes. "]},{"cell_type":"markdown","metadata":{"id":"AQKSYSrRYS06"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"08c7ecce80b69c620dde7c8919fa94d3baa2217d665b4d1ca74ed6f93561aaf3"}},"colab":{"provenance":[],"collapsed_sections":["eKszsldIYS01","yokv__0rYS03","YE8ITFsRYS04","Ih7oEGg3YS04","X55PEaQHYS05","nvCKnFZ3YS05","4zCs4vQzYS06"]}},"nbformat":4,"nbformat_minor":0}